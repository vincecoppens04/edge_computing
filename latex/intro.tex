Over the past decade, edge computing has become a central theme in distributed systems, driven by the rapid growth of connected devices and the rising demand for real-time, data-intensive applications. The idea is straightforward: instead of sending all data to distant cloud centres, part of the computation should occur close to where the data is produced. This reduces latency, limits bandwidth consumption, and improves the resilience of digital systems. At first sight, this seems sufficient to explain why edge computing has become relevant. Yet recent technological developments show that the field is moving beyond the classical model. Increasingly, the real innovation lies in the integration of local processing with cloud-level coordination, a convergence often described as \emph{Edge Cloud}.

This paper follows that evolution. We begin by outlining the foundations of edge computing, examining how modern AI workloads influence architectural choices and why the traditional notion of edge computation alone is no longer adequate. This leads to the broader edge--cloud continuum, where local inference, containerised services, and cloud-based orchestration operate together. The paper then compares several prominent frameworks that represent different approaches to this continuum, before presenting an experiment in which a small-scale edge--cloud architecture is designed and deployed using open-source tools. Finally, we reflect critically on the benefits, limitations, and remaining challenges in this domain.

The creation of this report combined classical literature research with practical experimentation. An introductory work on edge computing~\cite{lea-2023-edge-computing} was used alongside recent online documentation, technical standards, and open-source materials to structure and contextualise the foundational concepts. As the focus shifted to the edge--cloud continuum, the research drew increasingly on up-to-date sources, reflecting the rapid development of this field. Generative AI tools supported the writing and implementation phases by assisting with routine tasks such as rephrasing, structuring text, and generating code templates. All conceptual reasoning, source validation, and architectural decisions were made independently, and the AI assistance served only to streamline the production of the final report.

Together, these elements provide both a theoretical overview and a concrete demonstration of how edge computing is evolving. The aim of this paper is not only to describe this transition, but to make it accessible and operational for readers encountering the topic for the first time.