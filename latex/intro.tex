Over the past decade, edge computing has become a central theme in distributed systems, driven by the rapid growth of connected devices and the rising demand for real-time, data-intensive applications. The idea is straightforward: instead of sending all data to distant cloud centres, part of the computation should occur close to where the data is produced. This reduces latency, limits bandwidth consumption, and improves the resilience of digital systems. At first sight, this seems sufficient to explain why edge computing has become relevant. Yet recent technological developments show that the field is moving beyond the classical model. Increasingly, the real innovation lies in the integration of local processing with cloud-level coordination, a convergence often described as \emph{Edge Cloud}.

This paper follows that evolution. We begin by outlining the foundations of edge computing, drawing in the early stages on Perry Lea’s \textit{Edge Computing: A Friendly Introduction} as a conceptual backbone. Building on this, we examine how modern AI workloads influence architectural choices, and why the traditional notion of edge computing alone is no longer adequate. This leads to the broader edge--cloud continuum, where local inference, containerised services, and cloud-based orchestration operate together. The paper then compares several prominent frameworks that represent different approaches to this continuum, before presenting an experiment in which a small-scale edge--cloud architecture is designed and deployed using open-source tools. Finally, we reflect critically on the benefits, limitations, and remaining challenges in this domain.

The creation of this report combined classical literature research with practical experimentation. Perry Lea’s \textit{Edge Computing: A Friendly Introduction}\footnote{\bibentry{lea-2023-edge-computing}} served as an initial guide for structuring the foundational concepts presented in this report. As the focus shifted towards the broader edge--cloud continuum, the research drew on a range of recent online documentation, technical standards, and open-source materials, reflecting the fast-moving nature of this domain. Generative AI tools, in particular ChatGPT, supported the writing and development process by assisting with repetitive tasks such as rephrasing, structuring text, and producing code templates during implementation. All conceptual reasoning, validation of sources, and architectural decisions were made independently, and the AI assistance served only to streamline the production of the final report.

Together, these elements provide both a theoretical overview and a concrete demonstration of how edge computing is evolving. The aim of this paper is not only to describe this transition, but to make it accessible and operational for readers encountering the topic for the first time.